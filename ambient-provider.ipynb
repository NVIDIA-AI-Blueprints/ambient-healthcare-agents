{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "89d7b0cd",
   "metadata": {},
   "source": [
    "![Ambient Provider](images/ambientprovider.png)\n",
    "\n",
    "# Ambient Provider Getting Started Guide\n",
    "\n",
    "# Table of Contents\n",
    "\n",
    "- [Ambient Provider](#ambient-provider)\n",
    "  - [Summary](#summary)\n",
    "  - [Key Capabilities](#key-capabilities)\n",
    "  - [System Architecture](#system-architecture)\n",
    "- [Getting Started](#getting-started)\n",
    "  - [Prerequisites at a Glance](#prerequisites-at-a-glance)\n",
    "  - [NGC Account](#ngc-account)\n",
    "  - [HW Requirements](#hw-requirements)\n",
    "  - [Docker Installation](#docker-installation)\n",
    "  - [NVIDIA NIM Deployment](#nvidia-nim-deployment)\n",
    "  - [Dataset Download](#dataset-download)\n",
    "  - [Installation](#installation)\n",
    "- [Using the Platform](#using-the-platform)\n",
    "    - [Basic workflow](#basic-workflow)\n",
    "    - [Advanced Features](#advanced-features)\n",
    "---\n",
    "\n",
    "# Important: Git Submodule Setup\n",
    "\n",
    "⚠️⚠️  **Before proceeding, make sure to pull the git submodule first:**  ⚠️ ⚠️ "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c88c055b",
   "metadata": {
    "vscode": {
     "languageId": "shellscript"
    }
   },
   "outputs": [],
   "source": [
    "!git submodule update --init --recursive"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "12c44aa8",
   "metadata": {},
   "source": [
    "# Summary\n",
    "Ambient Provider is a comprehensive platform that converts audio recordings of medical consultations into structured clinical notes. The system uses NVIDIA NIM (NVIDIA Inference Microservices) for accurate speech recognition with speaker diarization, combined with reasoning large language models to generate medical documentation."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6b3f0de4",
   "metadata": {},
   "source": [
    "# Prerequisite Setup\n",
    "There are two key components to this transcription workflow:\n",
    "1) The NVIDIA NIM ASR transcription services with diarization (Parakeet model)\n",
    "2) The [llama-3.3-nemotron-super-49b-v1](https://build.nvidia.com/nvidia/llama-3_3-nemotron-super-49b-v1) reasoning model. \n",
    "\n",
    "This getting started guide will help you set up the necessary hardware and api keys to be able to run the ambient provider developer example.\n",
    "\n",
    "### Prerequisites at a Glance\n",
    "The bullet points below highlight an overview of the steps in this getting started guide:\n",
    "- **Setup NGC account**: Setting up account to download resources\n",
    "- **Ensure valid HW**: Confirm your system has the required HW\n",
    "- **Install Docker & NVIDIA Container Toolkit**: Enable GPU support for containers\n",
    "- **Deploy NIM**: Launch NVIDIA NIM for ASR and diarization\n",
    "- **Install SW**: Clone the repository and establish environment\n",
    "\n",
    "### Key Requirements\n",
    "1. **Hardware Requirements**:\n",
    "   - NVIDIA GPU with 16GB+ VRAM (for NVIDIA Riva ie: NVIDIA RTX, T4, L4)\n",
    "\n",
    "2. **Software Requirements**:\n",
    "   - Docker & Docker Compose v2.0+\n",
    "   - Git (for cloning repository)\n",
    "\n",
    "3. **API Keys**:\n",
    "   - NVIDIA API Key (from NGC)\n",
    "   - Network access to NVIDIA Riva deployment\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bff6d2d3",
   "metadata": {},
   "source": [
    "### NGC Account\n",
    "Setup an account on [NGC](https://ngc.nvidia.com) using the procedure in the [NGC user guide](https://docs.nvidia.com/ngc/gpu-cloud/ngc-user-guide/index.html). \n",
    "\n",
    "This is needed in order to obtain the necessary NGC API key credentials required to pull the [Riva Speech Skills SDK](https://catalog.ngc.nvidia.com/orgs/nvidia/teams/riva/containers/riva-speech) container and access the cloud api endpoints.\n",
    "\n",
    "Once your NGC credentials and Cloud account is setup, follow the details below to obtain the NGC_API_KEY to be able to access the [NVIDIA API catalog endpoints](https://build.nvidia.com) (AI Models, etc.). \n",
    "\n",
    "### Generate an API Key\n",
    "To access NGC resources, you need an NGC API key:\n",
    "\n",
    "1. Visit [NGC Personal Key Generation](https://org.ngc.nvidia.com/setup/personal-keys)\n",
    "2. Create a new API key\n",
    "3. Ensure \"NGC Catalog\" is selected from the \"Services Included\" dropdown\n",
    "4. Copy the generated API key\n",
    "\n",
    "### Export the API Key\n",
    "Make the NGC API key available to Docker:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9885ca63",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "export NGC_API_KEY=<value>\n",
    "echo \"export NGC_API_KEY=<value>\" >> ~/.bashrc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "# For zsh:\n",
    "# echo \"export NGC_API_KEY=<value>\" >> ~/.zshrc"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "824d0e1d",
   "metadata": {},
   "source": [
    "## Prepare Your Machine\n",
    "\n",
    "### Docker\n",
    "Install [Docker](https://docs.docker.com/engine/install/) on your system.\n",
    "\n",
    "### NVIDIA Container Toolkit\n",
    "Install the [NVIDIA Container Toolkit](https://docs.nvidia.com/datacenter/cloud-native/container-toolkit/latest/install-guide.html#installing-the-nvidia-container-toolkit) to enable GPU support in Docker containers.\n",
    "\n",
    "After installing the toolkit, follow the instructions in the Configure Docker section in the NVIDIA Container Toolkit [documentation.](https://docs.nvidia.com/datacenter/cloud-native/container-toolkit/latest/install-guide.html#configuring-docker)\n",
    "\n",
    "### Verify Installation\n",
    "Test your setup with the following command:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "docker run --rm --runtime=nvidia --gpus all ubuntu nvidia-smi"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4a53682e",
   "metadata": {},
   "source": [
    "This should produce output similar to:\n",
    "```\n",
    "+-----------------------------------------------------------------------------------------+\n",
    "| NVIDIA-SMI 550.54.14              Driver Version: 550.54.14      CUDA Version: 12.4     |\n",
    "|-----------------------------------------+------------------------+----------------------+\n",
    "| GPU  Name                 Persistence-M | Bus-Id          Disp.A | Volatile Uncorr. ECC |\n",
    "| Fan  Temp   Perf          Pwr:Usage/Cap |           Memory-Usage | GPU-Util  Compute M. |\n",
    "|                                         |                        |               MIG M. |\n",
    "|=========================================+========================+======================|\n",
    "|   0  NVIDIA H100 80GB HBM3          On  |   00000000:1B:00.0 Off |                    0 |\n",
    "| N/A   36C    P0            112W /  700W |   78489MiB /  81559MiB |      0%      Default |\n",
    "|                                         |                        |             Disabled |\n",
    "+-----------------------------------------+------------------------+----------------------+\n",
    "```\n",
    "\n",
    "### Docker Login to NGC\n",
    "Authenticate with the NVIDIA Container Registry:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "echo \"$NGC_API_KEY\" | docker login nvcr.io --username '$oauthtoken' --password-stdin"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "86c2e614",
   "metadata": {},
   "source": [
    "# Quick Start\n",
    "\n",
    "## 1. Setup the virtual environment\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "cd ambientprovider\n",
    "uv venv \n",
    "uv sync\n",
    "cd ambient-scribe"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "055be00f",
   "metadata": {},
   "source": [
    "## 2. Download the medical conversation dataset\n",
    "The dataset example used in this workflow can be obtained from [Hugging Face](https://huggingface.co/datasets/yfyeung/medical) and consists of simulated patient-physician interactions. Download the dataset into a sub-directory called \"dataset\" that is parallel in scope to the ambient-scribe directory. Be sure to unzip the dataset.\n",
    "\n",
    "## 3. Bootstrap the environment\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "make bootstrap"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e5785b86",
   "metadata": {},
   "source": [
    "This command:\n",
    "- Creates necessary directories\n",
    "- Sets up environment files\n",
    "- Validates dependencies\n",
    "\n",
    "## 4. Deploy Riva and the Ambient Provider\n",
    "\n",
    "You have two options for deploying NVIDIA NIM:\n",
    "\n",
    "### Option 1: RIVA Integrated with Docker Compose (Recommended)\n",
    "The easiest way is to use the built-in NIM profile that's integrated with the application:\n",
    "\n",
    "#### Configure environment variables:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "# Edit the API configuration\n",
    "nano apps/api/.env\n",
    "\n",
    "# Add the following configuration:\n",
    "# NVIDIA API Configuration (Required)\n",
    "# NVIDIA_API_KEY=your_nvidia_api_key_here\n",
    "# RIVA_URI=parakeet-nim:50051\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "28dd7910",
   "metadata": {},
   "source": [
    "#### Deploy the dev environment:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "# Development with local NIM\n",
    "make dev-nim\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5ff44be5",
   "metadata": {},
   "source": [
    "### Option 2: Standalone NIM Deployment\n",
    "Deploy the Parakeet 1.1b English ASR model manually with speaker diarization support:\n",
    "\n",
    "This option is primarily if you intend to deploy the riva container on a separate machine as your ambient provider. If you deploy riva outside of the docker network of ambient provider on the same machine, you may experience difficulties communicating between your riva and ambient provider applications due to firewall rules.\n",
    "\n",
    "#### On the separate machine:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "# Set container configuration\n",
    "export CONTAINER_ID=parakeet-1-1b-ctc-en-us\n",
    "export NIM_TAGS_SELECTOR=\"name=parakeet-1-1b-ctc-en-us,mode=all\"\n",
    "\n",
    "# Launch the NIM container\n",
    "docker run -d --rm --name=$CONTAINER_ID \\\n",
    "   --runtime=nvidia \\\n",
    "   --gpus '\"device=0\"' \\\n",
    "   --shm-size=8GB \\\n",
    "   -e NGC_API_KEY \\\n",
    "   -e NIM_HTTP_API_PORT=9000 \\\n",
    "   -e NIM_GRPC_API_PORT=50051 \\\n",
    "   -p 9000:9000 \\\n",
    "   -p 50051:50051 \\\n",
    "   -e NIM_TAGS_SELECTOR \\\n",
    "   nvcr.io/nim/nvidia/$CONTAINER_ID:latest\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "919962d9",
   "metadata": {},
   "source": [
    "For Option 2, configure your environment to point to your separate Riva machine:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "# Edit the API configuration\n",
    "nano apps/api/.env\n",
    "\n",
    "# Add the following configuration:\n",
    "# NVIDIA API Configuration (Required)  \n",
    "# NVIDIA_API_KEY=your_nvidia_api_key_here\n",
    "# RIVA_URI=<YOUR_RIVA_IP>:50051\n",
    "\n",
    "# Then deploy without local NIM\n",
    "make dev\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9cefe71d",
   "metadata": {},
   "source": [
    "#### Verify NIM Deployment\n",
    "Check that the NIM container is running:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "docker ps | grep parakeet\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "970b7a6d",
   "metadata": {},
   "source": [
    "You should see output similar to:\n",
    "```\n",
    "a1b2c3d4e5f6   nvcr.io/nim/nvidia/parakeet-1-1b-ctc-en-us:latest   \"/opt/nvidia/nvidia_…\"   2 minutes ago   Up 2 minutes   0.0.0.0:9000->9000/tcp, 0.0.0.0:50051->50051/tcp   parakeet-1-1b-ctc-en-us\n",
    "```\n",
    "\n",
    "The NIM will be accessible at:\n",
    "- **HTTP API**: http://localhost:9000\n",
    "- **gRPC API**: localhost:50051\n",
    "\n",
    "> **Note:** After starting the NIM container, check the container logs to ensure you see a message indicating that Riva is running and listening on port 9000. If you do not see this message, the Riva NIM may still be starting up. You can view the logs with:\n",
    ">\n",
    "> ```bash\n",
    "> docker logs -f $CONTAINER_ID\n",
    "> ```\n",
    ">\n",
    "> Wait until you see confirmation that the service is running on port 9000 before proceeding.\n",
    "\n",
    "## 5. Access the applications\n",
    "- **UI**: http://localhost:5173\n",
    "- **API Documentation**: http://localhost:8000/api/docs\n",
    "- **Health Check**: http://localhost:8000/api/health\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "baf304d5",
   "metadata": {},
   "source": [
    "## 6. Enable port access\n",
    "- **Brev**: If your cloud service provider enables exposing a port through the UI like in brev, you may specify to expose TCP/UDP traffic to port 5173 for this quick start guide. \n",
    "- **ngrok**: If you cannot expose ports directly, you can use [ngrok](https://ngrok.com/) to create a secure tunnel to your local development environment.\n",
    "\n",
    "### Using ngrok for remote access\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "# Install ngrok (if not already installed)\n",
    "sudo snap install ngrok\n",
    "\n",
    "# Add your ngrok authtoken (get from ngrok.com dashboard)\n",
    "ngrok config add-authtoken <YOUR_NGROK_AUTHTOKEN>\n",
    "\n",
    "# Expose your local port (e.g., 5173 for the UI)\n",
    "ngrok http 5173\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2317ef7b",
   "metadata": {},
   "source": [
    "# Using the Platform\n",
    "![ambientprovider UI](./images/ambient_provider_ui.png)\n",
    "\n",
    "## Basic Workflow\n",
    "\n",
    "1. **Upload Audio File**:\n",
    "   - Drag and drop an audio file (MP3, WAV, M4A, FLAC)\n",
    "   - Supported formats are automatically validated\n",
    "   - Maximum file size: 100MB (configurable)\n",
    "\n",
    "2. **Transcription Process**:\n",
    "   - Audio is converted to 16kHz mono WAV format\n",
    "   - NVIDIA Riva processes with speaker diarization\n",
    "   - Transcript segments are created with timestamps and speaker tags\n",
    "\n",
    "3. **Select Note Template**:\n",
    "   - Choose from available templates:\n",
    "     - **SOAP Default**: Standard Subjective, Objective, Assessment, Plan format\n",
    "     - **Progress Note**: For follow-up visits\n",
    "     - **Custom templates**: Created by your organization\n",
    "\n",
    "4. **Generate Medical Note**:\n",
    "   - AI processes the transcript using the selected template\n",
    "   - Real-time progress is shown with processing traces\n",
    "   - Note sections are generated and displayed incrementally\n",
    "\n",
    "5. **Edit and Refine**:\n",
    "   - Use the rich text editor to modify content\n",
    "   - Citations automatically link note content to transcript segments\n",
    "   - Autocomplete suggests content from the transcript\n",
    "\n",
    "6. **Export and Save**:\n",
    "   - Copy note to clipboard\n",
    "   - Save for future reference\n",
    "   - Export in various formats\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b942ccda",
   "metadata": {},
   "source": [
    "## How to Convert Between Streaming and Offline Transcription\n",
    "To switch between streaming and offline transcription modes, you need to update both the frontend and backend environment configuration files:\n",
    "\n",
    "1. **Frontend**:  \n",
    "   - Go into your frontend environment file (e.g., `.env`).\n",
    "   - Find the setting that enables streaming (e.g., `VITE_ENABLE_STREAMING=true`) and change it to `false`:\n",
    "     ```\n",
    "     VITE_ENABLE_STREAMING=false\n",
    "     ```\n",
    "\n",
    "2. **Backend**:  \n",
    "   - Open your backend environment file (e.g., `.env`).\n",
    "   - Change `ENABLE_STREAMING=true` to `ENABLE_STREAMING=false`.\n",
    "   - Update the Riva model name to use the offline model by replacing the word `streaming` with `offline` in the `RIVA_MODEL` variable. For example:\n",
    "     ```\n",
    "     ENABLE_STREAMING=false\n",
    "     RIVA_MODEL=parakeet-1.1b-en-US-asr-offline-silero-vad-sortformer\n",
    "     ```\n",
    "   - Make sure to restart both the frontend and backend services after making these changes. If you have a dev deployment the system will restart automatically.\n",
    "\n",
    "## Use hosted NIM\n",
    "To use the hosted NIM (NVIDIA Inference Microservice) instead of a self-hosted Riva deployment, you need to update your backend environment configuration:\n",
    "\n",
    "1. **Set `SELF_HOSTED` to `false`**  \n",
    "   In your backend `.env` file, change:\n",
    "   ```\n",
    "   SELF_HOSTED=false\n",
    "   ```\n",
    "\n",
    "2. **Update the Riva Function ID**  \n",
    "   Replace the `RIVA_FUNCTION_ID` value with the function ID provided by NVIDIA for your hosted NIM instance:\n",
    "   ```\n",
    "   RIVA_FUNCTION_ID=your_hosted_nim_function_id_here\n",
    "   ```\n",
    "\n",
    "3. **Set the Riva URI to the NVIDIA GRP URL**  \n",
    "   Update the `RIVA_URI` to point to the NVIDIA hosted endpoint, for example:\n",
    "   ```\n",
    "   RIVA_URI=grp.nvidia.com:443\n",
    "   ```\n",
    "\n",
    "Make sure to restart your backend service after making these changes for them to take effect.\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
