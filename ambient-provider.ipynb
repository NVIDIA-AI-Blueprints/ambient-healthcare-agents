{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "89d7b0cd",
   "metadata": {},
   "source": [
    "![Ambient Provider](./assets/ambientprovider.png)\n",
    "\n",
    "# Ambient Provider Getting Started Guide\n",
    "\n",
    "# Table of Contents\n",
    "\n",
    "- [Ambient Provider](#ambient-provider)\n",
    "  - [Summary](#summary)\n",
    "  - [Key Capabilities](#key-capabilities)\n",
    "  - [System Architecture](#system-architecture)\n",
    "- [Getting Started](#getting-started)\n",
    "  - [Prerequisites at a Glance](#prerequisites-at-a-glance)\n",
    "  - [NGC Account](#ngc-account)\n",
    "  - [HW Requirements](#hw-requirements)\n",
    "  - [Docker Installation](#docker-installation)\n",
    "  - [NVIDIA NIM Deployment](#nvidia-nim-deployment)\n",
    "  - [Dataset Download](#dataset-download)\n",
    "  - [Installation](#installation)\n",
    "- [Using the Platform](#using-the-platform)\n",
    "    - [Basic workflow](#basic-workflow)\n",
    "    - [Advanced Features](#advanced-features)\n",
    "---\n",
    "\n",
    "# Important: Git Submodule Setup\n",
    "\n",
    "⚠️⚠️  **Before proceeding, make sure to pull the git submodule first:**  ⚠️ ⚠️ "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "45325f7a-1a82-4b6d-a32d-6772a45b5fd7",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Navigate to the ambient-healthcare-agents directory\n",
    "%cd ~/ambient-healthcare-agents/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c88c055b",
   "metadata": {
    "vscode": {
     "languageId": "shellscript"
    }
   },
   "outputs": [],
   "source": [
    "!git submodule update --init --recursive"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "14bdc7b8-6c2c-4429-b0eb-3e7c320d8941",
   "metadata": {},
   "source": [
    "If you elect to run these commands directly from within the jupyter notebook. Please enable scrolling for cell outputs to ensure clear visualization."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "12c44aa8",
   "metadata": {},
   "source": [
    "# Summary\n",
    "Ambient Provider is a comprehensive platform that converts audio recordings of medical consultations into structured clinical notes. The system uses NVIDIA NIM (NVIDIA Inference Microservices) for accurate speech recognition with speaker diarization, combined with reasoning large language models to generate medical documentation."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6b3f0de4",
   "metadata": {},
   "source": [
    "# Prerequisite Setup\n",
    "There are two key components to this transcription workflow:\n",
    "1) The NVIDIA NIM ASR transcription services with diarization (Parakeet model)\n",
    "2) The [llama-3.3-nemotron-super-49b-v1](https://build.nvidia.com/nvidia/llama-3_3-nemotron-super-49b-v1) reasoning model. \n",
    "\n",
    "This getting started guide will help you set up the necessary hardware and api keys to be able to run the ambient provider developer example.\n",
    "\n",
    "### Prerequisites at a Glance\n",
    "The bullet points below highlight an overview of the steps in this getting started guide:\n",
    "- **Setup NGC account**: Setting up account to download resources\n",
    "- **Ensure valid HW**: Confirm your system has the required HW\n",
    "- **Install Docker & NVIDIA Container Toolkit**: Enable GPU support for containers\n",
    "- **Deploy NIM**: Launch NVIDIA NIM for ASR and diarization\n",
    "- **Install SW**: Clone the repository and establish environment\n",
    "\n",
    "### Key Requirements\n",
    "1. **Hardware Requirements**:\n",
    "   - NVIDIA GPU with 16GB+ VRAM (for NVIDIA Riva ie: NVIDIA RTX, T4, L4)\n",
    "\n",
    "2. **Software Requirements**:\n",
    "   - Docker & Docker Compose v2.0+\n",
    "   - Git (for cloning repository)\n",
    "   - npm (Node Package Manager, with Node > 20)\n",
    "\n",
    "3. **API Keys**:\n",
    "   - NVIDIA API Key (from NGC)\n",
    "   - Network access to NVIDIA Riva deployment\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bff6d2d3",
   "metadata": {},
   "source": [
    "### NGC Account\n",
    "Setup an account on [NGC](https://ngc.nvidia.com) using the procedure in the [NGC user guide](https://docs.nvidia.com/ngc/gpu-cloud/ngc-user-guide/index.html). \n",
    "\n",
    "This is needed in order to obtain the necessary NGC API key credentials required to pull the [Riva Speech Skills SDK](https://catalog.ngc.nvidia.com/orgs/nvidia/teams/riva/containers/riva-speech) container and access the cloud api endpoints.\n",
    "\n",
    "Once your NGC credentials and Cloud account is setup, follow the details below to obtain the NGC_API_KEY to be able to access the [NVIDIA API catalog endpoints](https://build.nvidia.com) (AI Models, etc.). \n",
    "\n",
    "### Generate an API Key\n",
    "To access NGC resources, you need an NGC API key:\n",
    "\n",
    "1. Visit [NGC Personal Key Generation](https://org.ngc.nvidia.com/setup/personal-keys)\n",
    "2. Create a new API key\n",
    "3. Ensure \"NGC Catalog\" is selected from the \"Services Included\" dropdown\n",
    "4. Copy the generated API key\n",
    "\n",
    "### Export the API Key\n",
    "Make the NGC API key available to Docker:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9885ca63",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ[\"NGC_API_KEY\"] = \"<your-api-key>\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "004ee5c1-b315-4ca7-9149-0d60a5f8aa8a",
   "metadata": {},
   "source": [
    "## Prepare Your Machine\n",
    "\n",
    "### Docker\n",
    "Install [Docker](https://docs.docker.com/engine/install/) on your system. Check to ensure the installation worked:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4331e473-4459-4989-bf16-1b7be2a7cc56",
   "metadata": {},
   "outputs": [],
   "source": [
    "!docker -v"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "824d0e1d",
   "metadata": {},
   "source": [
    "### NVIDIA Container Toolkit\n",
    "Install the [NVIDIA Container Toolkit](https://docs.nvidia.com/datacenter/cloud-native/container-toolkit/latest/install-guide.html#installing-the-nvidia-container-toolkit) to enable GPU support in Docker containers.\n",
    "\n",
    "After installing the toolkit, follow the instructions in the Configure Docker section in the NVIDIA Container Toolkit [documentation.](https://docs.nvidia.com/datacenter/cloud-native/container-toolkit/latest/install-guide.html#configuring-docker)\n",
    "\n",
    "### Verify Installation\n",
    "Test your setup with the following command:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de3782d7",
   "metadata": {
    "scrolled": true,
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "!docker run --rm --runtime=nvidia --gpus all ubuntu nvidia-smi"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4a53682e",
   "metadata": {},
   "source": [
    "This should produce output similar to:\n",
    "```\n",
    "+-----------------------------------------------------------------------------------------+\n",
    "| NVIDIA-SMI 550.54.14              Driver Version: 550.54.14      CUDA Version: 12.4     |\n",
    "|-----------------------------------------+------------------------+----------------------+\n",
    "| GPU  Name                 Persistence-M | Bus-Id          Disp.A | Volatile Uncorr. ECC |\n",
    "| Fan  Temp   Perf          Pwr:Usage/Cap |           Memory-Usage | GPU-Util  Compute M. |\n",
    "|                                         |                        |               MIG M. |\n",
    "|=========================================+========================+======================|\n",
    "|   0  NVIDIA H100 80GB HBM3          On  |   00000000:1B:00.0 Off |                    0 |\n",
    "| N/A   36C    P0            112W /  700W |   78489MiB /  81559MiB |      0%      Default |\n",
    "|                                         |                        |             Disabled |\n",
    "+-----------------------------------------+------------------------+----------------------+\n",
    "```\n",
    "\n",
    "### Docker Login to NGC\n",
    "Authenticate with the NVIDIA Container Registry:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "869a0ac8",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "!echo \"$NGC_API_KEY\" | docker login nvcr.io --username '$oauthtoken' --password-stdin"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f8e36785",
   "metadata": {},
   "source": [
    "### Checking Docker Storage Location\n",
    "\n",
    "Before we start with this notebook, we need to check the Docker storage location of the Brev instance. Since self deploying the NIMs will require 325 GB of disk space for the Docker related artifacts, we need to make sure the docker storage is specified to a location with enough disk space.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ca0dd7c",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "# view the disk space of the Brev instance you are using,\n",
    "# you should see a partition /ephemeral with enough space (more than 325 GB)\n",
    "!df -h"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a8d3c51",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "# next view the content of the docker service file\n",
    "!cat /etc/docker/daemon.json"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5c462d02",
   "metadata": {},
   "source": [
    "If the \"data-root\" is not specified, or specified to a partition that does not have enough disk space, modify the /etc/docker/daemon.json file so that it has `\"data-root\": \"/ephemeral\"`:\n",
    "```\n",
    "{\n",
    " ...,\n",
    " \"data-root\": \"/ephemeral\"\n",
    "}\n",
    "```\n",
    "\n",
    "Open the terminal, and run \n",
    "```\n",
    "sudo nano /etc/docker/daemon.json \n",
    "```\n",
    "\n",
    "to open the file for editing.\n",
    "\n",
    "For example:\n",
    "```\n",
    "{\n",
    "    \"default-runtime\": \"nvidia\",\n",
    "    \"mtu\": 1500,\n",
    "    \"runtimes\": {\n",
    "        \"nvidia\": {\n",
    "            \"args\": [],\n",
    "            \"path\": \"nvidia-container-runtime\"\n",
    "        }\n",
    "    },\n",
    "    \"data-root\": \"/ephemeral\"\n",
    "}\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f94c499",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "# view the content of the docker service file to make sure it has the correct setting\n",
    "!cat /etc/docker/daemon.json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "270deb2c",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "# then restart the docker service\n",
    "!sudo systemctl restart docker"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e5f8c440-4a39-4944-aab4-b8a077e461d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ensure the new volume read+execute access\n",
    "!sudo chmod 755 /ephemeral/"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "86c2e614",
   "metadata": {},
   "source": [
    "# Quick Start\n",
    "\n",
    "## 1. Setup the virtual environment\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "46667e6e-de8a-42cf-9968-76e4740f89f8",
   "metadata": {},
   "source": [
    "If the uv package manager is not installed, please install with the command below:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3773c806-9cbf-4dbc-8275-e9cf9b92e2cc",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "!curl -LsSf https://astral.sh/uv/install.sh | sh #sudo snap install uv (if within terminal)\n",
    "import os\n",
    "os.environ[\"PATH\"] = os.path.expanduser(\"~/.local/bin\") + \":\" + os.environ[\"PATH\"]\n",
    "!uv --version"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7702efec-b3d9-40d5-8f19-f46718d8efdc",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "!sudo apt-get update\n",
    "!sudo apt-get install -y portaudio19-dev"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7bf444ef-a077-4aad-9d7d-c69cbb9593a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "%cd ambient-provider"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb132675",
   "metadata": {
    "scrolled": true,
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "!uv python install 3.13\n",
    "!uv python pin 3.13\n",
    "\n",
    "!uv venv --clear\n",
    "!uv sync"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d1ed62e-c730-41a7-88b6-384dd6d470c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "%pwd\n",
    "%cd ambient-scribe"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "055be00f",
   "metadata": {},
   "source": [
    "## 2. Download the medical conversation dataset\n",
    "The dataset example used in this workflow can be obtained from [Hugging Face](https://huggingface.co/datasets/yfyeung/medical) and consists of simulated patient-physician interactions. Download the dataset onto the machine that will be used to visualize the UI. You do not need to add this to the repository. \n",
    "\n",
    "For example, If you host this develoepr exmaple on brev but are accessing the UI from a PC, please download the dataset to that PC directly. \n",
    "\n",
    "Be sure to untar the audio.tar.gz from within the dataset. Specifically, you should use the command tar -xzvf audio.tar.gz to obtain the audio folder.\n",
    "\n",
    "Later, once the UI is deployed, you will be able to select files from this folder from within the UI. \n",
    "\n",
    "\n",
    "## 3. Bootstrap the environment\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "85660967-e6f5-45db-ad3d-a6b9d7010912",
   "metadata": {},
   "source": [
    "The following command:\n",
    "- Creates necessary directories\n",
    "- Sets up environment files\n",
    "- Validates dependencies\n",
    "\n",
    "> **Note:** After running make bootstrap, you may notice a WARNING to fill in the .env file located at apps/api/.env. This file is created during the make bootstrap command. The WARNING indicates you must fill in the parameters as specified below before proceeding to the make dev-nim command. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d6f27246-2165-4a15-834a-88ff94bc2434",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. Check Node.js version (must be >= 20)\n",
    "!node --version"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8dd2db41",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# 2. If not installed or version < 20, install nvm and use Node.js 20\n",
    "!curl -fsSL https://deb.nodesource.com/setup_20.x | sudo -E bash -\n",
    "!sudo apt-get install -y nodejs\n",
    "!export NVM_DIR=\"$HOME/.nvm\" && [ -s \"$NVM_DIR/nvm.sh\" ] && . \"$NVM_DIR/nvm.sh\" && nvm install 20 && nvm use 20"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "05d32710-c48e-4fe8-880d-2ae066330895",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 3. Check node and npm availability\n",
    "!node --version\n",
    "!npm --version"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1cb44df2",
   "metadata": {
    "scrolled": true,
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "# This command may take a few minutes to setup the npm packages\n",
    "!make bootstrap"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e5785b86",
   "metadata": {},
   "source": [
    "## 4. Deploy Riva and the Ambient Provider\n",
    "\n",
    "You have two options for deploying NVIDIA NIM:\n",
    "\n",
    "### Option 1: RIVA Integrated with Docker Compose (Recommended)\n",
    "The easiest way is to use the built-in NIM profile that's integrated with the application:\n",
    "\n",
    "#### Configure environment variables:\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9810ad2c-f720-4676-b430-cf029bf9418c",
   "metadata": {},
   "source": [
    "If you do not see the hidden .env file in jupyter lab, please modify the file within a terminal session. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e476bac7",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "# Edit the API configuration\n",
    "# nano apps/api/.env\n",
    "\n",
    "# Add the following configuration:\n",
    "# NVIDIA API Configuration (Required)\n",
    "# NVIDIA_API_KEY=your_nvidia_api_key_here\n",
    "# RIVA_URI=parakeet-nim:50051"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "28dd7910",
   "metadata": {},
   "source": [
    "#### Deploy the dev environment:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8bcb4bec",
   "metadata": {
    "scrolled": true,
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "# Development with local NIM. This will take 5-10 minutes for the NIMs to standup\n",
    "!make dev-nim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bcc41679-dbad-4429-bd2f-775d7b843cda",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Health Check for Riva and LLM NIMs\n",
    "import requests\n",
    "\n",
    "llm_url = \"http://localhost:8001\"\n",
    "try:\n",
    "    resp = requests.get(llm_url + \"/v1/\")\n",
    "    print(f'LLM ready')\n",
    "except Exception as e:\n",
    "    print(f'LLM not ready')\n",
    "\n",
    "# Check Riva ASR\n",
    "riva_url = \"http://localhost:9000\"\n",
    "try:\n",
    "    resp = requests.get(riva_url + \"/v1/health\")\n",
    "    print(f'Riva ready')\n",
    "except Exception as e:\n",
    "    print(f'Riva not ready')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "618470e9-4799-4f1e-8d96-44db38fd5999",
   "metadata": {},
   "outputs": [],
   "source": [
    "!make dev"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5ff44be5",
   "metadata": {},
   "source": [
    "### Option 2: Standalone NIM Deployment\n",
    "Deploy the Parakeet 1.1b English ASR model manually with speaker diarization support:\n",
    "\n",
    "This option is primarily if you intend to deploy the riva container on a separate machine as your ambient provider. If you deploy riva outside of the docker network of ambient provider on the same machine, you may experience difficulties communicating between your riva and ambient provider applications due to firewall rules.\n",
    "\n",
    "#### On the separate machine:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cce63ebc",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "# Set container configuration\n",
    "export CONTAINER_ID=parakeet-1-1b-ctc-en-us\n",
    "export NIM_TAGS_SELECTOR=\"name=parakeet-1-1b-ctc-en-us,mode=all\"\n",
    "\n",
    "# Launch the NIM container\n",
    "docker run -d --rm --name=$CONTAINER_ID \\\n",
    "   --runtime=nvidia \\\n",
    "   --gpus '\"device=0\"' \\\n",
    "   --shm-size=8GB \\\n",
    "   -e NGC_API_KEY \\\n",
    "   -e NIM_HTTP_API_PORT=9000 \\\n",
    "   -e NIM_GRPC_API_PORT=50051 \\\n",
    "   -p 9000:9000 \\\n",
    "   -p 50051:50051 \\\n",
    "   -e NIM_TAGS_SELECTOR \\\n",
    "   nvcr.io/nim/nvidia/$CONTAINER_ID:latest"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "454a4b74",
   "metadata": {},
   "source": [
    "If you are self hosting the LLM reasonign NIM as well. Please follow the following documentation. https://build.nvidia.com/nvidia/llama-3_3-nemotron-super-49b-v1/deploy."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "919962d9",
   "metadata": {},
   "source": [
    "For Option 2, configure your environment to point to your separate Riva machine:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ad2f331",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "# Edit the API configuration\n",
    "# nano apps/api/.env\n",
    "\n",
    "# Add the following configuration:\n",
    "# NVIDIA API Configuration (Required)  \n",
    "# NVIDIA_API_KEY=your_nvidia_api_key_here\n",
    "# RIVA_URI=<YOUR_RIVA_IP>:50051"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "880fd65b-fc0e-4a1e-807c-9758b3220bcf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Then deploy without local NIM\n",
    "# make dev"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9cefe71d",
   "metadata": {},
   "source": [
    "#### Verify NIM Deployment\n",
    "Check that the NIM container is running:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "03aaa5b9",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "docker ps | grep parakeet"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "970b7a6d",
   "metadata": {},
   "source": [
    "You should see output similar to:\n",
    "```\n",
    "a1b2c3d4e5f6   nvcr.io/nim/nvidia/parakeet-1-1b-ctc-en-us:latest   \"/opt/nvidia/nvidia_…\"   2 minutes ago   Up 2 minutes   0.0.0.0:9000->9000/tcp, 0.0.0.0:50051->50051/tcp   parakeet-1-1b-ctc-en-us\n",
    "```\n",
    "\n",
    "The NIM will be accessible at:\n",
    "- **HTTP API**: http://localhost:9000\n",
    "- **gRPC API**: localhost:50051\n",
    "\n",
    "> **Note:** After starting the NIM container, check the container logs to ensure you see a message indicating that Riva is running and listening on port 9000. If you do not see this message, the Riva NIM may still be starting up. You can view the logs with:\n",
    ">\n",
    "> ```bash\n",
    "> docker logs -f $CONTAINER_ID\n",
    "> ```\n",
    ">\n",
    "> Wait until you see confirmation that the service is running on port 9000 before proceeding.\n",
    "\n",
    "## 5. Access the applications\n",
    "Please note if you are using brev, please follow step 6 to either expose the port as a secure link or create an ngrok tunnel. \n",
    "\n",
    "- **UI**: http://localhost:5173\n",
    "- **API Documentation**: http://localhost:8000/api/docs\n",
    "- **Health Check**: http://localhost:8000/api/health\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "baf304d5",
   "metadata": {},
   "source": [
    "## 6. Enable port access\n",
    "- **Brev**: If your cloud service provider enables exposing a port through the UI like in brev, you may specify to expose TCP/UDP traffic to port 5173 for this quick start guide. \n",
    "- **ngrok**: If you cannot expose ports directly, you can use [ngrok](https://ngrok.com/) to create a secure tunnel to your local development environment.\n",
    "\n",
    "### Using ngrok for remote access\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24e9fdad",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "# Install ngrok (if not already installed)\n",
    "sudo snap install ngrok\n",
    "\n",
    "# Add your ngrok authtoken (get from ngrok.com dashboard)\n",
    "ngrok config add-authtoken <YOUR_NGROK_AUTHTOKEN>\n",
    "\n",
    "# Expose your local port (e.g., 5173 for the UI)\n",
    "ngrok http 5173"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2317ef7b",
   "metadata": {},
   "source": [
    "# Using the Platform\n",
    "\n",
    "## Basic Workflow\n",
    "\n",
    "1. **Upload Audio File**:\n",
    "   - Drag and drop an audio file (MP3, WAV, M4A, FLAC)\n",
    "   - Supported formats are automatically validated\n",
    "   - Maximum file size: 100MB (configurable)\n",
    "\n",
    "2. **Transcription Process**:\n",
    "   - Audio is converted to 16kHz mono WAV format\n",
    "   - NVIDIA Riva processes with speaker diarization\n",
    "   - Transcript segments are created with timestamps and speaker tags\n",
    "\n",
    "3. **Select Note Template**:\n",
    "   - Choose from available templates:\n",
    "     - **SOAP Default**: Standard Subjective, Objective, Assessment, Plan format\n",
    "     - **Progress Note**: For follow-up visits\n",
    "     - **Custom templates**: Created by your organization\n",
    "\n",
    "4. **Generate Medical Note**:\n",
    "   - AI processes the transcript using the selected template\n",
    "   - Real-time progress is shown with processing traces\n",
    "   - Note sections are generated and displayed incrementally\n",
    "\n",
    "5. **Edit and Refine**:\n",
    "   - Use the rich text editor to modify content\n",
    "   - Citations automatically link note content to transcript segments\n",
    "   - Autocomplete suggests content from the transcript\n",
    "\n",
    "6. **Export and Save**:\n",
    "   - Copy note to clipboard\n",
    "   - Save for future reference\n",
    "   - Export in various formats\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "37147149-a46a-430f-9153-9d99f3af96ef",
   "metadata": {},
   "source": [
    "## How to Convert Between Streaming and Offline Transcription\n",
    "To switch between streaming and offline transcription modes, you need to update both the frontend and backend environment configuration files:\n",
    "\n",
    "1. **Frontend**:  \n",
    "   - Go into your frontend environment file (e.g., `apps/ui/.env`).\n",
    "   - Find the setting that enables streaming (e.g., `VITE_ENABLE_STREAMING=true`) and change it to `false`:\n",
    "     ```\n",
    "     VITE_ENABLE_STREAMING=false\n",
    "     ```\n",
    "\n",
    "2. **Backend**:  \n",
    "   - Open your backend environment file (e.g., `apps/api/.env`).\n",
    "   - Change `ENABLE_STREAMING=true` to `ENABLE_STREAMING=false`.\n",
    "   - Update the Riva model name to use the offline model by replacing the word `streaming` with `offline` in the `RIVA_MODEL` variable. For example:\n",
    "     ```\n",
    "     ENABLE_STREAMING=false\n",
    "     RIVA_MODEL=parakeet-1.1b-en-US-asr-offline-silero-vad-sortformer\n",
    "     ```\n",
    "   - Make sure to restart both the frontend and backend services after making these changes. If you have a dev deployment the system will restart automatically.\n",
    "\n",
    "3. **Reload**:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "74fbc4c8-311e-46b0-ad9f-c97fc0d748b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "!make down"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f3f76c3c-32d0-4d45-bb42-1cddd2dfef46",
   "metadata": {},
   "outputs": [],
   "source": [
    "!make dev"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b942ccda",
   "metadata": {},
   "source": [
    "## Use hosted NIM\n",
    "To use the hosted NIM (NVIDIA Inference Microservice) instead of a self-hosted Riva deployment, you need to update your backend environment configuration:\n",
    "\n",
    "1. **Set `SELF_HOSTED` to `false`**  \n",
    "   In your backend `.env` file, change:\n",
    "   ```\n",
    "   SELF_HOSTED=false\n",
    "   ```\n",
    "\n",
    "2. **Update the Riva Function ID**  \n",
    "   Replace the `RIVA_FUNCTION_ID` value with the function ID provided by NVIDIA for your hosted NIM instance:\n",
    "   ```\n",
    "   RIVA_FUNCTION_ID=your_hosted_nim_function_id_here\n",
    "   ```\n",
    "\n",
    "3. **Set the Riva URI to the NVIDIA GRP URL**  \n",
    "   Update the `RIVA_URI` to point to the NVIDIA hosted endpoint, for example:\n",
    "   ```\n",
    "   RIVA_URI=grp.nvidia.com:443\n",
    "   ```\n",
    "\n",
    "Make sure to restart your backend service after making these changes for them to take effect.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "850fbc3e-ef30-4da8-88ee-4d5cb1f547a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "!make down"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ec432ed-913d-44e6-9eb9-daca9114b910",
   "metadata": {},
   "outputs": [],
   "source": [
    "!make dev"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
