{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "cf7f3c6d",
   "metadata": {},
   "source": [
    "# Ambient Patient \n",
    "This notebook helps you get started with the [Ambient Healthcare Agent for Patients](https://github.com/NVIDIA-AI-Blueprints/ambient-patient) repository, where we have an example implementation of a healthcare agent assisting with the patient intake process via voice interactions powered by NVIDIA ACE Controller, and safeguarded by NVIDIA NeMo Guardrails.\n",
    "\n",
    "## Prerequisites\n",
    "- [Docker Compose](https://docs.docker.com/compose/install/)\n",
    "- [NVIDIA Container Toolkit](https://docs.nvidia.com/datacenter/cloud-native/container-toolkit/latest/install-guide.html)\n",
    "- [NVIDIA API Key](https://build.nvidia.com) This notebook uses NVIDIA NIM microservices hosted on build.nvidia.com for the majority of services. A NVIDIA API Key is required.\n",
    "- [NGC API Key](https://docs.nvidia.com/ngc/latest/ngc-private-registry-user-guide.html#ngc-api-keys) A NGC API Key is required to utilize the various NGC assets needed in this Developer Example.\n",
    "\n",
    "## NIMs Utilized\n",
    "### What is a NIM?\n",
    "NVIDIA NIM provides containers to self-host GPU-accelerated inferencing microservices for pretrained and customized AI models across clouds, data centers, and RTX™ AI PCs and workstations. NIM microservices expose industry-standard APIs for simple integration into AI applications, development frameworks, and workflows and optimize response latency and throughput for each combination of foundation model and GPU. Learn more about NIMs at https://developer.nvidia.com/nim.\n",
    "### NIMs in Ambient Patients\n",
    "- meta/llama-3.3-70b-instruct\n",
    "- nvidia/llama-3.1-nemoguard-8b-content-safety (Optional for NeMo Guardrails)\n",
    "- nvidia/llama-3.1-nemoguard-8b-topic-control (Optional for NeMo Guardrails)\n",
    "- nvidia/magpie-tts-multilingual\n",
    "- nvidia/parakeet-ctc-1.1b-asr\n",
    "\n",
    "## Hardware Requirements\n",
    "This notebook deploys all NIMs utilized in Ambient Healthcare Agents for Patients locally on this instance.\n",
    "\n",
    "Running this notebook requires:\n",
    "### Disk Space\n",
    "\n",
    "302 GB of disk space\n",
    "\n",
    "### GPU Requirement\n",
    "\n",
    "Use | Service(s)| Recommended GPU* \n",
    "--- | --- | --- \n",
    "[RIVA ASR NIM](https://build.nvidia.com/nvidia/parakeet-ctc-1_1b-asr/modelcard) | `nvidia/parakeet-ctc-1_1b-asr` |  1 x various options including L40, A100, and more (see [modelcard](https://build.nvidia.com/nvidia/parakeet-ctc-1_1b-asr/modelcard))\n",
    "[RIVA TTS NIM](https://build.nvidia.com/nvidia/magpie-tts-multilingual/modelcard) | `nvidia/magpie-tts-multilingual` | 1 x various options including L40, A100, and more (see [modelcard](https://build.nvidia.com/nvidia/parakeet-ctc-1_1b-asr/modelcard)) \n",
    "Instruct Model for Agentic Orchestration | `llama-3.3-70b-instruct` | 2 x H100 80GB <br /> or <br />4 x A100 80GB\n",
    "[NemoGuard Content Safety Model](https://build.nvidia.com/nvidia/llama-3_1-nemoguard-8b-content-safety/modelcard) (Optional for Enabling NeMo Guardrails) | `nvidia/llama-3_1-nemoguard-8b-content-safety` | 1x options including A100, H100, L40S, A6000\n",
    "[NemoGuard Topic Control Model](https://build.nvidia.com/nvidia/llama-3_1-nemoguard-8b-topic-control/modelcard) (Optional for Enabling NeMo Guardrails) | `nvidia/llama-3_1-nemoguard-8b-topic-control` | 1x options including A100, H100, L40S, A6000\n",
    "**Total** | Entire Ambient Healthcare Agent for Patients  | 8 x A100 80GB <br /> or other combinations of the above\n",
    "\n",
    "*For details on optimized configurations for LLMs, please see the documentation [Supported Models for NVIDIA NIM for LLMs](https://docs.nvidia.com/nim/large-language-models/latest/supported-models.html).\n",
    "\n",
    "Alternatively, if you're interested in utilizing the public NVIDIA NIM microservices hosted on build.nvidia.com for all microservices, follow the [docker compose deploy using public endpoints](https://github.com/NVIDIA-AI-Blueprints/ambient-patient/blob/main/docs/docker-compose-deploy-using-public-endpoints.md) documentation outside of this notebook. \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "31d2cdac",
   "metadata": {},
   "source": [
    "# Checking Docker Storage Location\n",
    "\n",
    "Before we start with this notebook, we need to check the Docker storage location of the Brev instance. Since self deploying the NIMs will require 302 GB of disk space for the Docker related artifacts, we need to make sure the docker storage is specified to a location with enough disk space.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "61a02e0a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# view the disk space of the Brev instance you are using,\n",
    "# you should see a partition /ephemeral with enough space (more than 302 GB)\n",
    "!df -h"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e765ae1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# next view the content of the docker service file\n",
    "!cat /etc/docker/daemon.json"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "be05ebb1",
   "metadata": {},
   "source": [
    "If the \"data-root\" is not specified, or specified to a partition that does not have enough disk space, modify the /etc/docker/daemon.json file so that it has `\"data-root\": \"/ephemeral\"`:\n",
    "```\n",
    "{\n",
    " ...,\n",
    " \"data-root\": \"/ephemeral\"\n",
    "}\n",
    "```\n",
    "\n",
    "Open the terminal, and run \n",
    "```\n",
    "sudo nano /etc/docker/daemon.json \n",
    "```\n",
    "\n",
    "to open the file for editing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "02a0d2b0-3049-4207-b18a-8b7da96f560c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# view the content of the docker service file to make sure it has the correct setting\n",
    "!cat /etc/docker/daemon.json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f53c58a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# then restart the docker service\n",
    "!sudo systemctl restart docker"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e69f920f",
   "metadata": {},
   "source": [
    "### Update Submodule for `ambient-patient`\n",
    "We should currently be in the repo `ambient-healthcare-agents`. Both `ambient-patient` and `ambient-provider` are submodules of the repo. Since we would like to go through the content in `ambient-patient`, first let's initialize and update the submodules to get the submodules' content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2faa44c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# this should show the current directory as `.../ambient-healthcare-agents`\n",
    "%pwd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea9c481a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# update the submodules\n",
    "!git submodule init && git submodule update --remote --recursive"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5b97f202",
   "metadata": {},
   "source": [
    "### Log in to NGC\n",
    "First, authenticate Docker with nvcr.io."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c31fca1-b089-455f-b7f1-5c5b7deedb4e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import subprocess\n",
    "import os\n",
    "\n",
    "NGC_API_KEY = input(\"Enter your NGC API key: \")\n",
    "cmd = f\"echo {NGC_API_KEY} | docker login nvcr.io -u '$oauthtoken' --password-stdin\"\n",
    "result = subprocess.run(cmd, shell=True, capture_output=True, text=True)\n",
    "print(result.stdout)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cf04ee89-8a24-4033-84bb-4f92115e8880",
   "metadata": {},
   "source": [
    "You should see `Login Succeeded`."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8bb07f55",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "## Getting Started \n",
    "### 1. Set Environment Variables for Agent Backend\n",
    "Read through this section [1. Edit the vars.env file to set environment variables](https://github.com/NVIDIA-AI-Blueprints/ambient-patient/tree/main/agent#1-edit-the-varsenv-file-to-set-environment-variables) in the agent/README to configure each of the environment variables needed in [agent/vars.env](ambient-patient/agent/vars.env) for bringing up the agent backend.\n",
    "\n",
    "- Since we are going to self host the agent LLM NIM, set both of the AGENT_LLM_BASE_URL and AGENT_LLM_MODEL differently:\n",
    "    ```sh\n",
    "    AGENT_LLM_BASE_URL=\"http://agent-instruct-llm:8000/v1\"\n",
    "    AGENT_LLM_MODEL=\"meta/llama-3.3-70b-instruct\"\n",
    "    ```\n",
    "- In order to utilize the NemoGuard NIMs for Nemo Guardrails around your agent LLM, set the config path for NeMo Guardrails:\n",
    "    ```sh\n",
    "    NEMO_GUARDRAILS_CONFIG_PATH=nmgr-config-store/patient-intake-nemoguard-self-hosted-nim\n",
    "    ```\n",
    "    Note the differences in base_url and model_name in the config.yml files for directories `patient-intake-nemoguard-self-hosted-nim` and `patient-intake-nemoguard`.\n",
    "\n",
    "Now, open [agent/vars.env](ambient-patient/agent/vars.env) and edit your variables, then save the file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0517a8f7-e98c-45db-8c00-476359772777",
   "metadata": {},
   "outputs": [],
   "source": [
    "# navigate to ambient-patient\n",
    "%cd ambient-patient\n",
    "%pwd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32645320",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make sure you have opened vars.env and finished editing the variables.\n",
    "# Now, let's check the vars.env file.\n",
    "!cat agent/vars.env"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f30f7178-ac8d-4314-b1bb-d29332588591",
   "metadata": {},
   "source": [
    "Check the output to make sure that in your [agent/vars.env](ambient-patient/agent/vars.env) file that you have set \n",
    "```\n",
    "AGENT_LLM_BASE_URL=\"http://agent-instruct-llm:8000/v1\"\n",
    "AGENT_LLM_MODEL=\"meta/llama-3.3-70b-instruct\"\n",
    "```\n",
    "and \n",
    "```\n",
    "NEMO_GUARDRAILS_CONFIG_PATH=nmgr-config-store/patient-intake-nemoguard-self-hosted-nim\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9984a9eb-dbb3-4ed4-a496-9a0417e70dde",
   "metadata": {},
   "source": [
    "### 2. Deploy Agent LLM NIM Locally\n",
    "We will first bring up the LLM powering the agent."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b2c2007e-fa06-4e61-b47f-d512a6850933",
   "metadata": {},
   "outputs": [],
   "source": [
    "# set the GPU IDs for the meta/llama-3.3-70b-instruct NIM\n",
    "# since we are on a system of A100s, we use 4 GPUs\n",
    "# if you are on a different compute setup, set the GPU IDs accordingly\n",
    "os.environ['AGENT_LLM_GPU_ID'] = \"0,1,2,3\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2aa96041-85f8-4a71-b359-e7ce5f012ee3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# now run the NIM container, which will take a few minutes to pull\n",
    "try:\n",
    "    result = subprocess.run(\n",
    "    [\"docker\", \"compose\", \"-f\", \"agent/docker-compose.yaml\", \"up\", \"-d\", \"agent-instruct-llm\"],\n",
    "    stdout=subprocess.PIPE,\n",
    "    stderr=subprocess.PIPE,\n",
    "    text=True,\n",
    "    check=True\n",
    "    )\n",
    "    print(result.stdout[-1000:], flush=True)\n",
    "except subprocess.CalledProcessError as e:\n",
    "    print(e.stderr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0453eaa5-79f2-45a4-88b2-ae9f0aae3215",
   "metadata": {},
   "outputs": [],
   "source": [
    "# check that the container is running\n",
    "result = subprocess.run(\n",
    "    [\"docker\", \"ps\", \"--format\", \"table {{.Names}}\\t{{.Image}}\\t{{.Status}}\"],\n",
    "    stdout=subprocess.PIPE,\n",
    "    stderr=subprocess.PIPE,\n",
    "    text=True,\n",
    ")\n",
    "\n",
    "print(result.stdout)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5bdf55c2-c709-4dab-98cd-a776fdb1976e",
   "metadata": {},
   "source": [
    "You should see the name `agent-instruct-lm` in the result:\n",
    "\n",
    "NAMES  |  IMAGE | STATUS\n",
    "--- | --- | --- \n",
    "agent-instruct-llm  |nvcr.io/nim/meta/llama-3.3-70b-instruct:1.8.5   |Up About a minute (health: starting)\n",
    "\n",
    "Note: after the image is pulled, it should take less than 20 minutes for the status of the container to change from starting to healthy. You can continue to the next steps 3 and 4 while the status is health: starting."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c86ac917-9fc1-4289-b605-2f992b04a8e3",
   "metadata": {},
   "source": [
    "### 3. Deploy NemoGuard NIMs Locally\n",
    "   \n",
    "Since we would like to utilize the NemoGuard NIMs for guardrailing, and you have set `NEMO_GUARDRAILS_CONFIG_PATH=nmgr-config-store/patient-intake-nemoguard-self-hosted-nim` in your vars.env file, first set the GPU IDs for the NemoGuard NIMs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c4e37c74-0205-4593-bb22-3ef236a9aa50",
   "metadata": {},
   "outputs": [],
   "source": [
    "# set the GPU IDs for the NemoGuard NIMs utilized by NemoGuardrails\n",
    "# since we are on a system of A100s, specify the 5th and 6th GPU\n",
    "# if you are on a different compute setup, set the GPU IDs accordingly\n",
    "os.environ['NEMOGUARD_CONTENT_SAFETY_LLM_GPU_ID'] = \"4\"\n",
    "os.environ['NEMOGUARD_TOPIC_CONTRIL_LLM_GPU_ID'] = \"5\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0b6f8f75-92fc-46ec-a91e-d552042a2475",
   "metadata": {},
   "source": [
    "Next bring up the two NemoGuard NIMs deployment:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "add4c617-0b70-4ce4-8e1f-5f5632f20936",
   "metadata": {},
   "outputs": [],
   "source": [
    "# this will spin up both NIM containers:\n",
    "try:\n",
    "    result = subprocess.run(\n",
    "        [\"docker\", \"compose\", \"-f\", \"agent/docker-compose.yaml\", \"up\", \"-d\", \n",
    "         \"nemoguard-content-safety-llm\", \"nemoguard-topic-control-llm\"],\n",
    "        stdout=subprocess.PIPE,\n",
    "        stderr=subprocess.PIPE,\n",
    "        text=True,\n",
    "        check=True\n",
    "    )\n",
    "    print(result.stdout[-1000:], flush=True)\n",
    "except subprocess.CalledProcessError as e:\n",
    "    print(e.stderr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "163704f9-4ddf-48a0-8e03-59b605ba6f3e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# check that the containers are running\n",
    "result = subprocess.run(\n",
    "    [\"docker\", \"ps\", \"--format\", \"table {{.Names}}\\t{{.Image}}\\t{{.Status}}\"],\n",
    "    stdout=subprocess.PIPE,\n",
    "    stderr=subprocess.PIPE,\n",
    "    text=True,\n",
    ")\n",
    "\n",
    "print(result.stdout)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d1ec5c79-e61f-4a82-9333-84cc44afbb0a",
   "metadata": {},
   "source": [
    "You should see the new names `nemoguard-content-safety-llm` and `nemoguard-topic-control-llm` in the result:\n",
    "\n",
    "NAMES  | IMAGE                  |      STATUS\n",
    "--- | --- | --- \n",
    "nemoguard-content-safety-llm |nvcr.io/nim/nvidia/llama-3.1-nemoguard-8b-content-safety:1.10.1 |Up 7 minutes (healthy)\n",
    "nemoguard-topic-control-llm          |nvcr.io/nim/nvidia/llama-3.1-nemoguard-8b-topic-control:1.10.1  |Up 7 minutes (healthy)\n",
    "...\n",
    "\n",
    "Note: after the images are pulled, it should take about 5 minutes for the status of the containers to change from starting to healthy. You can continue to the next step 4 while the status is health: starting."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8dad180a",
   "metadata": {},
   "source": [
    "### 4. Deploy Agent Backend App Server\n",
    "\n",
    " We will be bringing up the `app-server` service in [agent/docker-compose.yaml](https://github.com/NVIDIA-AI-Blueprints/ambient-patient/blob/main/agent/docker-compose.yaml)\n",
    "\n",
    " Double check that your environment variables are set correctly according to \n",
    " [1. Set Environment Variables for Agent Backend](https://github.com/NVIDIA-AI-Blueprints/ambient-patient/tree/main/agent#1-edit-the-varsenv-file-to-set-environment-variables), then bring up the app-server service:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aa771e58",
   "metadata": {},
   "source": [
    "We will be bringing up the `app-server` service in [agent/docker-compose.yaml](https://github.com/NVIDIA-AI-Blueprints/ambient-patient/tree/main/agent/docker-compose.yaml)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "88d00334",
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    result = subprocess.run(\n",
    "    [\"docker\", \"compose\", \"-f\", \"agent/docker-compose.yaml\", \"up\", \"--build\", \"-d\", \"app-server\"],\n",
    "    stdout=subprocess.PIPE,\n",
    "    stderr=subprocess.PIPE,\n",
    "    text=True,\n",
    "    check=True\n",
    "    )\n",
    "    print(result.stdout[-1000:], flush=True)\n",
    "except subprocess.CalledProcessError as e:\n",
    "    print(e.stderr)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bf75fd23",
   "metadata": {},
   "source": [
    "The build should take a few minutes. Note that after building the image for the first time, for bringing up this service again, if nothing in the source files changes and you don't need to rebuild the image, you could remove `--build` from the command:\n",
    "\n",
    "```sh\n",
    "docker compose -f agent/docker-compose.yaml up -d app-server\n",
    "```\n",
    "\n",
    "After the build finishes, check that the container is running:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "235bb024",
   "metadata": {},
   "outputs": [],
   "source": [
    "result = subprocess.run(\n",
    "    [\"docker\", \"ps\", \"--format\", \"table {{.Names}}\\t{{.Image}}\\t{{.Status}}\"],\n",
    "    stdout=subprocess.PIPE,\n",
    "    stderr=subprocess.PIPE,\n",
    "    text=True,\n",
    ")\n",
    "\n",
    "print(result.stdout)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cf0651b2",
   "metadata": {},
   "source": [
    "you should see the new name `app-server-healthcare-assistant` in the results:\n",
    "\n",
    "NAMES    |  IMAGE             |        STATUS\n",
    "--- | --- | --- \n",
    "app-server-healthcare-assistant |  app-server-healthcare-assistant:latest         |                  Up 14 seconds\n",
    "...\n",
    "\n",
    "Please wait 20 seconds. We could ping the FastAPI application that we just created by sending a request now.\n",
    "\n",
    "Note that after the meta/llama-3.3-70b-instruct LLM is up and running, the first requests may take longer than expected.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b152b670",
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "import json\n",
    "\n",
    "import requests\n",
    "data = {\n",
    " \"messages\": [\n",
    "    {\n",
    "      \"role\": \"user\",\n",
    "      \"content\": \"Hi there I want to check in.\"\n",
    "    }\n",
    "  ],\n",
    "  \n",
    "  \"max_tokens\": 256\n",
    "}\n",
    "\n",
    "url = \"http://0.0.0.0:8081/generate\"\n",
    "\n",
    "start_time = time.time()\n",
    "with requests.post(url, stream=True, json=data) as req:\n",
    "    for chunk in req.iter_lines():\n",
    "        raw_resp = chunk.decode(\"UTF-8\")\n",
    "        if not raw_resp:\n",
    "            continue\n",
    "        resp_dict = json.loads(raw_resp[6:])\n",
    "        resp_choices = resp_dict.get(\"choices\", [])\n",
    "        if len(resp_choices):\n",
    "            resp_str = resp_choices[0].get(\"message\", {}).get(\"content\", \"\")\n",
    "            print(resp_str, end =\"\")\n",
    "\n",
    "print(f\"--- {time.time() - start_time} seconds ---\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c322e9eb-f125-4815-9a5c-f05f023bb659",
   "metadata": {},
   "source": [
    "You should see a message welcoming you and asking for your name, such as: \n",
    "```\n",
    "Welcome to our clinic! I’m so glad you’re here. I’m the patient intake assistant and we’re going to do our best to help you feel better. Could you please tell me your name?\n",
    "--- 1.9298450946807861 seconds ---\n",
    "```\n",
    "\n",
    "You can experiment with the NeMo Guardrails functionality here. For example, asking \"Did my brother check in recently?\" is not allowed by the guardrails as it is asking for other patients' information, and you should get a response \"I'm sorry, I can't respond to that.\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b5d718c7-c48e-4da6-9b1f-323176daa5be",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = {\n",
    " \"messages\": [\n",
    "    {\n",
    "      \"role\": \"user\",\n",
    "      \"content\": \"Did my brother check in recently?\"\n",
    "    }\n",
    "  ],\n",
    "  \n",
    "  \"max_tokens\": 256\n",
    "}\n",
    "\n",
    "start_time = time.time()\n",
    "with requests.post(url, stream=True, json=data) as req:\n",
    "    for chunk in req.iter_lines():\n",
    "        raw_resp = chunk.decode(\"UTF-8\")\n",
    "        if not raw_resp:\n",
    "            continue\n",
    "        resp_dict = json.loads(raw_resp[6:])\n",
    "        resp_choices = resp_dict.get(\"choices\", [])\n",
    "        if len(resp_choices):\n",
    "            resp_str = resp_choices[0].get(\"message\", {}).get(\"content\", \"\")\n",
    "            print(resp_str, end =\"\")\n",
    "\n",
    "print(f\"--- {time.time() - start_time} seconds ---\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5cf63355-afcb-4629-8922-b611e3897bfa",
   "metadata": {},
   "source": [
    "After a prior message has been blocked by the guardrails, you could continue the conversation where you left off. But all interactions including ones resulting in \"I'm sorry, I can't respond to that.\" will be logged."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d4577f3-d48d-4a94-ae16-62bf49b216a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = {\n",
    " \"messages\": [\n",
    "    {\n",
    "      \"role\": \"user\",\n",
    "      \"content\": \"Okay let's continue. My name is Caroline.\"\n",
    "    }\n",
    "  ],\n",
    "  \n",
    "  \"max_tokens\": 256\n",
    "}\n",
    "\n",
    "start_time = time.time()\n",
    "with requests.post(url, stream=True, json=data) as req:\n",
    "    for chunk in req.iter_lines():\n",
    "        raw_resp = chunk.decode(\"UTF-8\")\n",
    "        if not raw_resp:\n",
    "            continue\n",
    "        resp_dict = json.loads(raw_resp[6:])\n",
    "        resp_choices = resp_dict.get(\"choices\", [])\n",
    "        if len(resp_choices):\n",
    "            resp_str = resp_choices[0].get(\"message\", {}).get(\"content\", \"\")\n",
    "            print(resp_str, end =\"\")\n",
    "\n",
    "print(f\"--- {time.time() - start_time} seconds ---\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "696de519",
   "metadata": {},
   "source": [
    "### 5. Set Environment Variables for the ace-controller Voice UI\n",
    "We will need to set the environment variables in [`ace-controller-voice-interface/ace_controller.env`](ambient-patient/ace-controller-voice-interface/ace_controller.env)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "44b811d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# view the content of the ace_controller.env file\n",
    "!cat ace-controller-voice-interface/ace_controller.env"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e8549a04",
   "metadata": {},
   "source": [
    "Follow this section [Setup API Keys and Configure Service Settings](https://github.com/NVIDIA-AI-Blueprints/ambient-patient/tree/main/ace-controller-voice-interface/README.md#setup-api-keys-and-configure-service-settings) in the ace-controller-voice-interface/README and set the variables in [`ace-controller-voice-interface/ace_controller.env`](https://github.com/NVIDIA-AI-Blueprints/ambient-patient/tree/main/ace-controller-voice-interface/ace_controller.env).\n",
    "\n",
    "- Set your API Keys\n",
    "- Since we're self hosting the RIVA ASR and TTS NIMs and not using the public endpoints, change the default   `CONFIG_PATH` to \n",
    "\n",
    "    ```sh\n",
    "    CONFIG_PATH=./configs/config_riva_self_hosting.yaml\n",
    "    ```\n",
    "\n",
    "Now, edit the [`ace-controller-voice-interface/ace_controller.env`](ambient-patient/ace-controller-voice-interface/ace_controller.env) file and save it before moving on."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a7df4ba9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# view the content of the ace_controller.env file before proceeding\n",
    "!cat ace-controller-voice-interface/ace_controller.env"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c83cfc80",
   "metadata": {},
   "source": [
    "### 6. Deploy RIVA NIMS\n",
    "First, set the GPU IDs for the RIVA NIMs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "74813329",
   "metadata": {},
   "outputs": [],
   "source": [
    "# since in the brev launchable with 8 x A100s, we could set the RIVA ASR and TTS NIMs to the 7th and 8th GPU\n",
    "# both NIMs could also be deployed on the same A100\n",
    "# if you are on a different compute setup, set the GPU IDs accordingly\n",
    "os.environ['RIVA_ASR_NIM_GPU_ID'] = \"6\"\n",
    "os.environ['RIVA_TTS_NIM_GPU_ID'] = \"7\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "828c18ca",
   "metadata": {},
   "source": [
    "Next deploy the RIVA NIMs. \n",
    "\n",
    "When in the Brev launchable, the RIVA NIMs should have prebuilt model profiles, but if not running in the Brev launchable and self deploying the RIVA NIMs, please see the [Known Issues](https://github.com/NVIDIA-AI-Blueprints/ambient-patient/blob/main/docs/known_issues.md) documentation on the RIVA TTS NIM known issue.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b2958492",
   "metadata": {},
   "outputs": [],
   "source": [
    "# bring up the RIVA ASR AND TTS NIMs\n",
    "try:\n",
    "    result = subprocess.run(\n",
    "        [\"docker\", \"compose\", \"--profile\", \"riva-nims-local\", \"-f\", \"ace-controller-voice-interface/docker-compose.yml\", \"up\", \"-d\"],\n",
    "        stdout=subprocess.PIPE,\n",
    "        stderr=subprocess.PIPE,\n",
    "        text=True,\n",
    "        check=True\n",
    "    )\n",
    "    print(result.stdout[-1000:], flush=True)\n",
    "except subprocess.CalledProcessError as e:\n",
    "    print(e.stderr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe0677a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# check that the containers are up and running\n",
    "result = subprocess.run(\n",
    "    [\"docker\", \"ps\", \"--format\", \"table {{.Names}}\\t{{.Image}}\\t{{.Status}}\"],\n",
    "    stdout=subprocess.PIPE,\n",
    "    stderr=subprocess.PIPE,\n",
    "    text=True,\n",
    ")\n",
    "\n",
    "print(result.stdout)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e1bf31dd",
   "metadata": {},
   "source": [
    "You should see the additional names `voice-agents-webrtc-riva-tts-magpie-1` and `voice-agents-webrtc-riva-asr-parakeet-1` in the result:\n",
    "\n",
    "NAMES     |        IMAGE       |         STATUS\n",
    "--- | --- | --- \n",
    "voice-agents-webrtc-riva-asr-parakeet-1  | nvcr.io/nim/nvidia/parakeet-1-1b-ctc-en-us:1.3.0  | Up 2 minutes (healthy)\n",
    "voice-agents-webrtc-riva-tts-magpie-1    | nvcr.io/nim/nvidia/magpie-tts-multilingual:1.3.0   |Up 2 minutes (health: starting)\n",
    "...\n",
    "\n",
    "Note: after the images are pulled, it should take about 8 minutes for the status of the containers to change from starting to healthy.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "621e7b17",
   "metadata": {},
   "source": [
    "### 7. Stand up a Turn Server\n",
    "When deploying on cloud providers such as Brev, a Turn server is needed. A Turn server is needed for WebRTC connections when clients are behind NATs or firewalls that prevent direct peer-to-peer communication. \n",
    "#### 7.1: Run the Turn server docker container\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "acd9b543",
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "# find out the external ip address of the instance\n",
    "HOST_IP_EXTERNAL = requests.get('https://ifconfig.me').text.strip()\n",
    "print(HOST_IP_EXTERNAL)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f241620c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# set the environment variable\n",
    "os.environ['HOST_IP_EXTERNAL'] = HOST_IP_EXTERNAL\n",
    "\n",
    "# run the turn server docker container\n",
    "try:\n",
    "    result = subprocess.run(\n",
    "    [\"docker\", \"run\", \"-d\", \"--name\", \"turn-server\", \"--network=host\", \"instrumentisto/coturn\",\n",
    "    \"-n\", \"--verbose\", \"--log-file=stdout\", \"--external-ip=\" + HOST_IP_EXTERNAL, \"--listening-ip=0.0.0.0\",\n",
    "    \"--lt-cred-mech\", \"--fingerprint\", \"--user=admin:admin\", \"--no-multicast-peers\", \"--realm=tokkio.realm.org\",\n",
    "    \"--min-port=51000\", \"--max-port=51010\"],\n",
    "    stdout=subprocess.PIPE,\n",
    "    stderr=subprocess.PIPE,\n",
    "    text=True,\n",
    "    check=True\n",
    "    )\n",
    "    print(result.stdout[-1000:], flush=True)\n",
    "except subprocess.CalledProcessError as e:\n",
    "    print(e.stderr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "beb22905",
   "metadata": {},
   "outputs": [],
   "source": [
    "# check that the container is running\n",
    "result = subprocess.run(\n",
    "    [\"docker\", \"ps\", \"--format\", \"table {{.Names}}\\t{{.Image}}\\t{{.Status}}\"],\n",
    "    stdout=subprocess.PIPE,\n",
    "    stderr=subprocess.PIPE,\n",
    "    text=True,\n",
    ")\n",
    "\n",
    "print(result.stdout)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b1242fe6",
   "metadata": {},
   "source": [
    "You should see the container named `turn-server` in the results:\n",
    "\n",
    "NAMES    |   IMAGE     |       STATUS\n",
    "--- | --- | --- \n",
    "turn-server    | instrumentisto/coturn   |         Up 25 seconds\n",
    "..."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e279d3a0",
   "metadata": {},
   "source": [
    "#### 7.2: Modify ace-controller app configuration\n",
    "Next, modify the `ace_controller.env` file and `config.ts` file under `ace-controller-voice-interface`. The `config.ts` file will be utilized in the docker build process for the ui-app container from the webrtc_ui example."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "223ceebd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# view the content of the existing ace-controller-voice-interface/ace_controller.env\n",
    "!cat ace-controller-voice-interface/ace_controller.env"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f08975d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# add three relevant env vars to ace-controller-voice-interface/ace_controller.env\n",
    "!echo -e \"\\n\\nTURN_USERNAME=admin\\nTURN_PASSWORD=admin\\nTURN_SERVER_URL=turn:$HOST_IP_EXTERNAL:3478\" >> ace-controller-voice-interface/ace_controller.env"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1da22092",
   "metadata": {},
   "outputs": [],
   "source": [
    "# check the modified content of the ace-controller-voice-interface/ace_controller.env\n",
    "!cat ace-controller-voice-interface/ace_controller.env"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "08be7f78",
   "metadata": {},
   "outputs": [],
   "source": [
    "# next check the content of the existing config.ts file\n",
    "!cat ace-controller-voice-interface/config.ts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "37a7a0c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# replace the ice server definition in config.ts\n",
    "!sed -i \"s/export const RTC_CONFIG = {};/export const RTC_CONFIG: ConstructorParameters<typeof RTCPeerConnection>[0] = {\\n    iceServers: [\\n      {\\n        urls: \\\"turn:$HOST_IP_EXTERNAL:3478\\\",\\n        username: \\\"admin\\\",\\n        credential: \\\"admin\\\",\\n      },\\n    ],\\n  };/\" ace-controller-voice-interface/config.ts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad32c522",
   "metadata": {},
   "outputs": [],
   "source": [
    "# next check the modified content of the config.ts file\n",
    "!cat ace-controller-voice-interface/config.ts"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e3800b38",
   "metadata": {},
   "source": [
    "#### 7.3: Expose ports on your cloud provider instance\n",
    "\n",
    "On the cloud provider instance, make sure the following ports are exposed:\n",
    "- 4400\n",
    "- 7860\n",
    "- 3478\n",
    "- 51000-51010 (this is from the range specified by the Turn server docker run command)\n",
    "\n",
    "If on Brev, make sure the ports have been exposed using the `TCP/UDP Ports` section in your web console's `Access` tab. In the end your section should look like this: https://github.com/NVIDIA-AI-Blueprints/ambient-patient/blob/main/docs/images/all_ports_exposed.png\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a0a2c0e2",
   "metadata": {},
   "source": [
    "### 8. Deploy the ace-controller Python App and WebRTC UI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8fef1887",
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    result = subprocess.run(\n",
    "    [\"docker\", \"compose\", \"--profile\", \"ace-controller\", \"-f\", \"ace-controller-voice-interface/docker-compose.yml\", \"up\", \"--build\", \"-d\"],\n",
    "    stdout=subprocess.PIPE,\n",
    "    stderr=subprocess.PIPE,\n",
    "    text=True,\n",
    "    check=True\n",
    "    )\n",
    "    print(result.stdout[-1000:], flush=True)\n",
    "except subprocess.CalledProcessError as e:\n",
    "    print(e.stderr)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b3fc196",
   "metadata": {},
   "source": [
    "\n",
    "The build should take a few minutes. Remove `--build` from the command if you haven't change any sources files and are bringing up the services again:\n",
    "```sh\n",
    "docker compose --profile ace-controller -f ace-controller-voice-interface/docker-compose.yml up -d\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f2fb685",
   "metadata": {},
   "outputs": [],
   "source": [
    "# check that the containers are running\n",
    "result = subprocess.run(\n",
    "    [\"docker\", \"ps\", \"--format\", \"table {{.Names}}\\t{{.Image}}\\t{{.Status}}\"],\n",
    "    stdout=subprocess.PIPE,\n",
    "    stderr=subprocess.PIPE,\n",
    "    text=True,\n",
    ")\n",
    "\n",
    "print(result.stdout)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "245565b6",
   "metadata": {},
   "source": [
    "\n",
    "You should see the new names `voice-agents-webrtc-ui-app-1` and `voice-agents-webrtc-python-app-1` in the docker ps results, for example:\n",
    "\n",
    "\n",
    "NAMES                  |            IMAGE                       |             STATUS\n",
    "--- | --- | ---\n",
    "voice-agents-webrtc-python-app-1       |   voice-agents-webrtc-python-app                                |    Up 6 minutes (healthy)\n",
    "voice-agents-webrtc-ui-app-1           |   voice-agents-webrtc-ui-app                                    |    Up 6 minutes\n",
    "turn-server                            |   instrumentisto/coturn                                         |    Up 10 minutes\n",
    "voice-agents-webrtc-riva-tts-magpie-1  |   nvcr.io/nim/nvidia/magpie-tts-multilingual:1.3.0               |   Up 25 minutes (healthy)\n",
    "voice-agents-webrtc-riva-asr-parakeet-1  | nvcr.io/nim/nvidia/parakeet-1-1b-ctc-en-us:1.3.0               |   Up 25 minutes (healthy)\n",
    "app-server-healthcare-assistant         |  app-server-healthcare-assistant:latest                           | Up 39 minutes\n",
    "nemoguard-content-safety-llm            |  nvcr.io/nim/nvidia/llama-3.1-nemoguard-8b-content-safety:1.10.1  | Up 50 minutes (healthy)\n",
    "nemoguard-topic-control-llm             |  nvcr.io/nim/nvidia/llama-3.1-nemoguard-8b-topic-control:1.10.1    |Up 50 minutes (healthy)\n",
    "agent-instruct-llm                      |  nvcr.io/nim/meta/llama-3.3-70b-instruct:1.8.5                 |  Up 59 minutes (healthy)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a679a370",
   "metadata": {},
   "source": [
    "### 9. Go to the Voice UI in your Web Browser\n",
    "\n",
    "First, to enable microphone access in Chrome, go to `chrome://flags/` in your Chrome browser, enable \"Insecure origins treated as secure\", add `http://<brev-instance-ip>:4400` to the list, and restart Chrome.\n",
    "\n",
    "You could find the brev instance ip by looking at the expose port section.\n",
    "\n",
    "Next, go to `http://<brev-instance-ip>:4400` in your browser to visit the voice UI. Upon loading, the page should look like the following: https://github.com/NVIDIA-AI-Blueprints/ambient-patient/blob/main/ace-controller-voice-interface/assets/ui_at_start.png\n",
    "\n",
    "Please note that after the meta/llama-3.3-70b-instruct LLM is up and running, the first requests may take longer than expected.\n",
    "\n",
    "#### Troubleshooting\n",
    "##### Permission Issue\n",
    "If you're getting an error `Cannot read properties of undefined (reading 'getUserMedia')`, that means you have not enabled microphone access in Chrome. Go to `chrome://flags/`, enable \"Insecure origins treated as secure\", add `http://<machine-ip>:4400` to the list, and restart Chrome.\n",
    "\n",
    "![](ambient-patient/docs/images/webpage_permission_error.png)\n",
    "\n",
    "##### Timeout Issue\n",
    "If you're getting a timeout issue where the button shows `Connecting...` and then \"WebRTC connection failed\", double check all the steps in the document. It's likely due to incorrect configurations.\n",
    "\n",
    "![](ambient-patient/docs/images/webrtc_connection_failed.png)\n",
    "\n",
    "After setting the correct configurations, make sure to **close the browser tab**, and open a new browser tab to access the application. If that doesn't seem to work, clear your browser cache and open the link again.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fa67f75f",
   "metadata": {},
   "source": [
    "### 10. Bring down the services\n",
    "Bring down all services specified in the agent backend"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e75f0336",
   "metadata": {},
   "outputs": [],
   "source": [
    "!docker compose -f agent/docker-compose.yaml down "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c0c1c491",
   "metadata": {},
   "source": [
    "Bring down all services specified in the ace-controller web UI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28b6bf27-6821-4812-99e2-871eb15cabc8",
   "metadata": {},
   "outputs": [],
   "source": [
    "!docker compose -f ace-controller-voice-interface/docker-compose.yml down\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
